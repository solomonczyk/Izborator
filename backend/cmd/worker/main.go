package main

import (
	"context"
	"encoding/json"
	"errors"
	"flag"
	"os"
	"os/signal"
	"strings"
	"sync"
	"syscall"
	"time"

	"github.com/joho/godotenv"
	"github.com/solomonczyk/izborator/internal/app"
	"github.com/solomonczyk/izborator/internal/config"
	"github.com/solomonczyk/izborator/internal/logger"
	"github.com/solomonczyk/izborator/internal/queue"
	"github.com/solomonczyk/izborator/internal/scraper"
)

func main() {
	// –ó–∞–≥—Ä—É–∑–∫–∞ .env —Ñ–∞–π–ª–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É, –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω)
	_ = godotenv.Load()

	// –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
	cfg, err := config.Load()
	if err != nil {
		panic(err)
	}

	// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
	application, err := app.NewWorkerApp(cfg)
	if err != nil {
		panic(err)
	}
	defer application.Close()

	log := application.Logger()

	// –§–ª–∞–≥–∏
	daemonMode := flag.Bool("daemon", false, "Run in daemon mode (scheduler)")
	testURL := flag.String("url", "", "Single URL scrape test")
	shopIDStr := flag.String("shop", "a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11", "Shop ID for single scrape")
	processRaw := flag.Bool("process", false, "Run processor once")
	batchSize := flag.Int("batch-size", 100, "Batch size for processing")
	reindex := flag.Bool("reindex", false, "Run full reindex once")
	discover := flag.Bool("discover", false, "Run catalog discovery once")

	flag.Parse()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// --- 1. –†–ï–ñ–ò–ú–´ ONE-OFF (–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫) ---

	if *testURL != "" {
		if *shopIDStr == "" {
			log.Fatal("Shop ID is required for url scrape", nil)
		}
		runSingleScrape(ctx, application, *testURL, *shopIDStr, log)
		return
	}

	if *processRaw {
		runProcessor(ctx, application, *batchSize, log)
		return
	}

	if *reindex {
		runReindex(ctx, application, log)
		return
	}

	if *discover {
		runCatalogDiscovery(ctx, application, log)
		return
	}

	// --- 2. –†–ï–ñ–ò–ú –î–ï–ú–û–ù–ê (–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è) ---

	if *daemonMode {
		log.Info("üöÄ Starting Worker Daemon...", nil)

		// –ö–∞–Ω–∞–ª—ã –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
		stop := make(chan os.Signal, 1)
		signal.Notify(stop, syscall.SIGINT, syscall.SIGTERM)

		// WaitGroup –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
		var wg sync.WaitGroup

		queueClient := application.QueueClient()
		if queueClient != nil && cfg.Queue.Topic != "" {
			wg.Add(1)
			go func() {
				defer wg.Done()
				runQueueConsumer(ctx, application, queueClient, cfg.Queue.Topic, log)
			}()
		} else {
			log.Info("Queue consumer disabled", map[string]interface{}{
				"queue_type": cfg.Queue.Type,
				"topic":      cfg.Queue.Topic,
			})
		}

		// –¢–∏–∫–µ—Ä—ã (–¢–∞–π–º–µ—Ä—ã)
		// –ü—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∑–∞–ø—É—Å–∫–∞–µ–º —á–∞—Å—Ç–æ (–∫–∞–∂–¥—ã–µ 30 —Å–µ–∫), —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –ø–æ–¥—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
		processTicker := time.NewTicker(30 * time.Second)
		defer processTicker.Stop()

		// –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–¥–∫–æ (–∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤), —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–ª—è—Ç—å —Ü–µ–Ω—ã
		// –î–ª—è —Ç–µ—Å—Ç–∞ –ø–æ—Å—Ç–∞–≤–∏–º 10 –º–∏–Ω—É—Ç, —á—Ç–æ–±—ã —Ç—ã —É–≤–∏–¥–µ–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ã—Å—Ç—Ä–µ–µ
		scrapeTicker := time.NewTicker(10 * time.Minute)
		defer scrapeTicker.Stop()

		// –ó–∞–ø—É—Å–∫ –≥–æ—Ä—É—Ç–∏–Ω—ã –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
		go func() {
			// –°—Ä–∞–∑—É –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–¥–µ–ª–∞–µ–º –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω –≤—Å–µ–≥–æ
			log.Info("Running initial startup tasks...", nil)
			wg.Add(4)
			go func() { defer wg.Done(); runCatalogDiscovery(ctx, application, log) }()      // –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–∞–ª–æ–≥–∞—Ö
			go func() { defer wg.Done(); runMonitoring(ctx, application, log) }()            // –°–∫—Ä–∞–ø–∏–Ω–≥ —Å–ø–∏—Å–∫–∞
			go func() { defer wg.Done(); runProcessor(ctx, application, *batchSize, log) }() // –ü—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
			go func() { defer wg.Done(); runReindex(ctx, application, log) }()               // –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è

			for {
				select {
				case <-processTicker.C:
					wg.Add(1)
					go func() {
						defer wg.Done()
						runProcessor(ctx, application, *batchSize, log)
					}()

				case <-scrapeTicker.C:
					log.Info("‚è∞ Scheduled scraping started", nil)
					wg.Add(3)
					go func() { defer wg.Done(); runCatalogDiscovery(ctx, application, log) }() // –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
					go func() { defer wg.Done(); runMonitoring(ctx, application, log) }()       // –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω
					go func() { defer wg.Done(); runReindex(ctx, application, log) }()          // –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è

				case <-ctx.Done():
					return
				}
			}
		}()

		// –ë–ª–æ–∫–∏—Ä—É–µ–º main, –ø–æ–∫–∞ –Ω–µ –ø—Ä–∏–¥–µ—Ç —Å–∏–≥–Ω–∞–ª —Å—Ç–æ–ø
		<-stop
		log.Info("Shutting down daemon...", nil)

		// –û—Ç–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤—Å–µ—Ö –≥–æ—Ä—É—Ç–∏–Ω
		cancel()

		// –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∏–∫–µ—Ä—ã
		processTicker.Stop()
		scrapeTicker.Stop()

		// –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á (graceful shutdown)
		shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), 30*time.Second)
		defer shutdownCancel()

		// –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á —Å —Ç–∞–π–º–∞—É—Ç–æ–º
		done := make(chan struct{})
		go func() {
			wg.Wait()
			close(done)
		}()

		select {
		case <-done:
			log.Info("‚úÖ Graceful shutdown completed - all tasks finished", nil)
		case <-shutdownCtx.Done():
			log.Warn("‚ö†Ô∏è Shutdown timeout reached, forcing exit", nil)
		}

		return
	}

	// –ï—Å–ª–∏ —Ñ–ª–∞–≥–æ–≤ –Ω–µ—Ç
	log.Info("Worker started (waiting for jobs...) - use -url to test scrape, -process to process raw data, or -daemon to start scheduler", map[string]interface{}{})
	flag.Usage()
}

// --- –•–ï–õ–ü–ï–†–´ ---

func runSingleScrape(ctx context.Context, app *app.App, url, shopID string, log *logger.Logger) {
	log.Info("Manual scrape started", map[string]interface{}{
		"url":     url,
		"shop_id": shopID,
	})

	// –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –º–∞–≥–∞–∑–∏–Ω–∞
	shopConfig, err := app.GetShopConfig(shopID)
	if err != nil {
		log.Fatal("Shop config not found", map[string]interface{}{
			"error":   err.Error(),
			"shop_id": shopID,
		})
	}

	log.Info("Shop config loaded", map[string]interface{}{
		"shop_name": shopConfig.Name,
		"base_url":  shopConfig.BaseURL,
	})

	// –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä
	rawProduct, err := app.ScraperService.ScrapeAndSave(ctx, url, shopConfig)
	if err != nil {
		log.Fatal("‚ùå Scrape & save failed", map[string]interface{}{
			"error": err.Error(),
			"url":   url,
		})
	}

	log.Info("‚úÖ SUCCESS! Product parsed & saved", map[string]interface{}{
		"name":     rawProduct.Name,
		"price":    rawProduct.Price,
		"currency": rawProduct.Currency,
		"brand":    rawProduct.Brand,
		"category": rawProduct.Category,
	})
}

func runProcessor(ctx context.Context, app *app.App, batchSize int, log *logger.Logger) {
	log.Info("üîÑ Processor tick", nil)
	count, err := app.ProcessorService.ProcessRawProducts(ctx, batchSize)
	if err != nil {
		log.Error("Processing failed", map[string]interface{}{"error": err.Error()})
	} else if count > 0 {
		log.Info("Processed items", map[string]interface{}{"count": count})
	}
}

func runReindex(ctx context.Context, app *app.App, log *logger.Logger) {
	log.Info("üîç Reindex tick", nil)
	// –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
	reindexCtx, cancel := context.WithTimeout(ctx, 10*time.Minute)
	defer cancel()
	
	if err := app.ReindexAllWithContext(reindexCtx); err != nil {
		log.Error("Reindex failed", map[string]interface{}{"error": err.Error()})
	} else {
		log.Info("‚úÖ Reindex completed successfully", nil)
	}
}

func runMonitoring(ctx context.Context, app *app.App, log *logger.Logger) {
	log.Info("üïµÔ∏è Checking for outdated prices...", nil)

	// –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å –±–æ–ª–µ–µ 6 —á–∞—Å–æ–≤
	// –ë–µ—Ä–µ–º –ø–∞—á–∫–∞–º–∏ –ø–æ 10 —à—Ç—É–∫, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–ø–∞–º–∏—Ç—å –º–∞–≥–∞–∑–∏–Ω—ã
	outdatedItems, err := app.ProductsService.GetURLsForRescrape(ctx, 6*time.Hour, 10)
	if err != nil {
		log.Error("Failed to get urls for rescrape", map[string]interface{}{"error": err.Error()})
		return
	}

	if len(outdatedItems) == 0 {
		log.Info("All prices are fresh ‚ú®", nil)
		return
	}

	log.Info("Found outdated items", map[string]interface{}{"count": len(outdatedItems)})

	// –û–±—Ö–æ–¥–∏–º –∏—Ö
	for _, item := range outdatedItems {
		log.Info("Rescraping item", map[string]interface{}{"url": item.URL, "shop_id": item.ShopID})

		// –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –º–∞–≥–∞–∑–∏–Ω–∞
		shopConfig, err := app.GetShopConfig(item.ShopID)
		if err != nil {
			log.Error("Shop config not found", map[string]interface{}{
				"shop_id": item.ShopID,
				"error":   err.Error(),
			})
			continue
		}

		// –ü–∞—Ä—Å–∏–º
		// ScrapeAndSave –æ–±–Ω–æ–≤–ª—è–µ—Ç existing product —á–µ—Ä–µ–∑ Processor.ProcessRawProducts (UPSERT)
		_, err = app.ScraperService.ScrapeAndSave(ctx, item.URL, shopConfig)
		if err != nil {
			log.Error("Rescrape failed", map[string]interface{}{
				"url":   item.URL,
				"error": err.Error(),
			})
		} else {
			log.Info("Rescraped successfully", map[string]interface{}{"url": item.URL})
		}

		// –í–µ–∂–ª–∏–≤–∞—è –ø–∞—É–∑–∞
		time.Sleep(5 * time.Second)
	}

	log.Info("‚úÖ Monitoring scrape completed", map[string]interface{}{"count": len(outdatedItems)})
}

func runCatalogDiscovery(ctx context.Context, app *app.App, log *logger.Logger) {
	log.Info("üîç Starting catalog discovery...", nil)

	// –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤
	shops, err := app.ScraperService.ListShops(ctx)
	if err != nil {
		log.Error("Failed to list shops", map[string]interface{}{"error": err.Error()})
		return
	}

	for _, shop := range shops {
		if !shop.Enabled {
			continue
		}

		// –ü–æ–ª—É—á–∞–µ–º URL –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
		catalogURL := shop.Selectors["catalog_url"]

		// –ï—Å–ª–∏ catalog_url –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å base_url –∫–∞–∫ —Ç–æ—á–∫—É –≤—Ö–æ–¥–∞
		if catalogURL == "" {
			log.Info("No catalog_url configured, trying base_url", map[string]interface{}{"shop": shop.Name})
			catalogURL = shop.BaseURL
		}

		if catalogURL == "" {
			log.Info("No catalog URL available, skipping", map[string]interface{}{"shop": shop.Name})
			continue
		}

		log.Info("Discovering products from catalog", map[string]interface{}{
			"shop":        shop.Name,
			"catalog_url": catalogURL,
		})

		// –ü–∞—Ä—Å–∏–º –∫–∞—Ç–∞–ª–æ–≥ (–º–∞–∫—Å–∏–º—É–º 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞ —Ä–∞–∑, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å)
		result, err := app.ScraperService.ParseCatalog(ctx, catalogURL, shop, 3)
		if err != nil {
			log.Error("Catalog parsing failed", map[string]interface{}{
				"shop":  shop.Name,
				"error": err.Error(),
			})
			continue
		}

		if result.TotalFound == 0 {
			log.Info("No products found in catalog", map[string]interface{}{"shop": shop.Name})
			continue
		}

		log.Info("Found products in catalog", map[string]interface{}{
			"shop":        shop.Name,
			"total_found": result.TotalFound,
		})

		// –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ URL –≤ –±–∞–∑—É –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
		savedCount := 0
		for _, productURL := range result.ProductURLs {
			// –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π RawProduct –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è URL
			rawProduct := &scraper.RawProduct{
				ShopID:    shop.ID,
				ShopName:  shop.Name,
				URL:       productURL,
				ParsedAt:  time.Now(),
				ScrapedAt: time.Now(),
			}

			// –ò–∑–≤–ª–µ–∫–∞–µ–º external_id –∏–∑ URL
			parts := strings.Split(productURL, "/")
			if len(parts) > 0 {
				rawProduct.ExternalID = parts[len(parts)-1]
			}

			// –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Ä–µ–∑ ScraperService (–æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ScrapeAndSave, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ raw_products)
			// –ù–æ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å URL, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ storage
			// –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –∑–∞–ø—É—Å—Ç–∏–º –±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
			_, err = app.ScraperService.ScrapeAndSave(ctx, productURL, shop)
			if err != nil {
				log.Error("Failed to scrape product from catalog", map[string]interface{}{
					"url":   productURL,
					"error": err.Error(),
				})
				continue
			}
			savedCount++

			// –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–æ–≤–∞—Ä–∞–º–∏
			time.Sleep(2 * time.Second)
		}

		log.Info("‚úÖ Catalog discovery completed", map[string]interface{}{
			"shop":  shop.Name,
			"found": result.TotalFound,
			"saved": savedCount,
		})

		// –ü–∞—É–∑–∞ –º–µ–∂–¥—É –º–∞–≥–∞–∑–∏–Ω–∞–º–∏
		time.Sleep(5 * time.Second)
	}
}


func runQueueConsumer(ctx context.Context, app *app.App, queueClient queue.Client, topic string, log *logger.Logger) {
	log.Info("Queue consumer started", map[string]interface{}{
		"topic": topic,
	})

	err := queueClient.Consume(ctx, topic, func(payload []byte) error {
		var raw scraper.RawProduct
		if err := json.Unmarshal(payload, &raw); err != nil {
			log.Error("Queue payload decode failed", map[string]interface{}{
				"topic": topic,
				"error": err.Error(),
			})
			return err
		}
		if app.ProcessorService == nil {
			return errors.New("processor service is not initialized")
		}
		return app.ProcessorService.ProcessRawProduct(ctx, &raw)
	})
	if err != nil && !errors.Is(err, context.Canceled) {
		log.Warn("Queue consumer stopped", map[string]interface{}{
			"topic": topic,
			"error": err.Error(),
		})
	}
}
