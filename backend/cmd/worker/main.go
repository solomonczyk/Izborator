package main

import (
	"context"
	"flag"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/joho/godotenv"
	"github.com/solomonczyk/izborator/internal/app"
	"github.com/solomonczyk/izborator/internal/config"
	"github.com/solomonczyk/izborator/internal/logger"
)

func main() {
	// –ó–∞–≥—Ä—É–∑–∫–∞ .env —Ñ–∞–π–ª–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É, –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω)
	_ = godotenv.Load()

	// –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
	cfg, err := config.Load()
	if err != nil {
		panic(err)
	}

	// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
	application, err := app.NewWorkerApp(cfg)
	if err != nil {
		panic(err)
	}
	defer application.Close()

	log := application.Logger()

	// –§–ª–∞–≥–∏
	daemonMode := flag.Bool("daemon", false, "Run in daemon mode (scheduler)")
	testURL := flag.String("url", "", "Single URL scrape test")
	shopIDStr := flag.String("shop", "a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11", "Shop ID for single scrape")
	processRaw := flag.Bool("process", false, "Run processor once")
	batchSize := flag.Int("batch-size", 100, "Batch size for processing")
	reindex := flag.Bool("reindex", false, "Run full reindex once")

	flag.Parse()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// --- 1. –†–ï–ñ–ò–ú–´ ONE-OFF (–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫) ---

	if *testURL != "" {
		if *shopIDStr == "" {
			log.Fatal("Shop ID is required for url scrape", nil)
		}
		runSingleScrape(ctx, application, *testURL, *shopIDStr, log)
		return
	}

	if *processRaw {
		runProcessor(ctx, application, *batchSize, log)
		return
	}

	if *reindex {
		runReindex(ctx, application, log)
		return
	}

	// --- 2. –†–ï–ñ–ò–ú –î–ï–ú–û–ù–ê (–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è) ---

	if *daemonMode {
		log.Info("üöÄ Starting Worker Daemon...", nil)

		// –ö–∞–Ω–∞–ª—ã –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
		stop := make(chan os.Signal, 1)
		signal.Notify(stop, syscall.SIGINT, syscall.SIGTERM)

		// –¢–∏–∫–µ—Ä—ã (–¢–∞–π–º–µ—Ä—ã)
		// –ü—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∑–∞–ø—É—Å–∫–∞–µ–º —á–∞—Å—Ç–æ (–∫–∞–∂–¥—ã–µ 30 —Å–µ–∫), —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –ø–æ–¥—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
		processTicker := time.NewTicker(30 * time.Second)
		defer processTicker.Stop()

		// –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–¥–∫–æ (–∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤), —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–ª—è—Ç—å —Ü–µ–Ω—ã
		// –î–ª—è —Ç–µ—Å—Ç–∞ –ø–æ—Å—Ç–∞–≤–∏–º 10 –º–∏–Ω—É—Ç, —á—Ç–æ–±—ã —Ç—ã —É–≤–∏–¥–µ–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ã—Å—Ç—Ä–µ–µ
		scrapeTicker := time.NewTicker(10 * time.Minute)
		defer scrapeTicker.Stop()

		// –ó–∞–ø—É—Å–∫ –≥–æ—Ä—É—Ç–∏–Ω—ã –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
		go func() {
			// –°—Ä–∞–∑—É –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–¥–µ–ª–∞–µ–º –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω –≤—Å–µ–≥–æ
			log.Info("Running initial startup tasks...", nil)
			runMonitoring(ctx, application, log) // –°–∫—Ä–∞–ø–∏–Ω–≥ —Å–ø–∏—Å–∫–∞
			runProcessor(ctx, application, *batchSize, log)  // –ü—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
			runReindex(ctx, application, log)    // –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è

			for {
				select {
				case <-processTicker.C:
					runProcessor(ctx, application, *batchSize, log)

				case <-scrapeTicker.C:
					log.Info("‚è∞ Scheduled scraping started", nil)
					runMonitoring(ctx, application, log)
					// –ü–æ—Å–ª–µ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –ª–æ–≥–∏—á–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å
					runReindex(ctx, application, log)

				case <-ctx.Done():
					return
				}
			}
		}()

		// –ë–ª–æ–∫–∏—Ä—É–µ–º main, –ø–æ–∫–∞ –Ω–µ –ø—Ä–∏–¥–µ—Ç —Å–∏–≥–Ω–∞–ª —Å—Ç–æ–ø
		<-stop
		log.Info("Shutting down daemon...", nil)
		processTicker.Stop()
		scrapeTicker.Stop()
		return
	}

	// –ï—Å–ª–∏ —Ñ–ª–∞–≥–æ–≤ –Ω–µ—Ç
	log.Info("Worker started (waiting for jobs...) - use -url to test scrape, -process to process raw data, or -daemon to start scheduler", map[string]interface{}{})
	flag.Usage()
}

// --- –•–ï–õ–ü–ï–†–´ ---

func runSingleScrape(ctx context.Context, app *app.App, url, shopID string, log *logger.Logger) {
	log.Info("Manual scrape started", map[string]interface{}{
		"url":     url,
		"shop_id": shopID,
	})

	// –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –º–∞–≥–∞–∑–∏–Ω–∞
	shopConfig, err := app.GetShopConfig(shopID)
	if err != nil {
		log.Fatal("Shop config not found", map[string]interface{}{
			"error":   err.Error(),
			"shop_id": shopID,
		})
	}

	log.Info("Shop config loaded", map[string]interface{}{
		"shop_name": shopConfig.Name,
		"base_url":  shopConfig.BaseURL,
	})

	// –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä
	rawProduct, err := app.ScraperService.ScrapeAndSave(ctx, url, shopConfig)
	if err != nil {
		log.Fatal("‚ùå Scrape & save failed", map[string]interface{}{
			"error": err.Error(),
			"url":   url,
		})
	}

	log.Info("‚úÖ SUCCESS! Product parsed & saved", map[string]interface{}{
		"name":     rawProduct.Name,
		"price":    rawProduct.Price,
		"currency": rawProduct.Currency,
		"brand":    rawProduct.Brand,
		"category": rawProduct.Category,
	})
}

func runProcessor(ctx context.Context, app *app.App, batchSize int, log *logger.Logger) {
	log.Info("üîÑ Processor tick", nil)
	count, err := app.ProcessorService.ProcessRawProducts(ctx, batchSize)
	if err != nil {
		log.Error("Processing failed", map[string]interface{}{"error": err.Error()})
	} else if count > 0 {
		log.Info("Processed items", map[string]interface{}{"count": count})
	}
}

func runReindex(ctx context.Context, app *app.App, log *logger.Logger) {
	log.Info("üîç Reindex tick", nil)
	if err := app.ReindexAll(); err != nil {
		log.Error("Reindex failed", map[string]interface{}{"error": err.Error()})
	} else {
		log.Info("‚úÖ Reindex completed successfully", nil)
	}
}

func runMonitoring(ctx context.Context, app *app.App, log *logger.Logger) {
	log.Info("üïµÔ∏è Checking for outdated prices...", nil)

	// –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å –±–æ–ª–µ–µ 6 —á–∞—Å–æ–≤
	// –ë–µ—Ä–µ–º –ø–∞—á–∫–∞–º–∏ –ø–æ 10 —à—Ç—É–∫, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–ø–∞–º–∏—Ç—å –º–∞–≥–∞–∑–∏–Ω—ã
	outdatedItems, err := app.ProductsService.GetURLsForRescrape(ctx, 6*time.Hour, 10)
	if err != nil {
		log.Error("Failed to get urls for rescrape", map[string]interface{}{"error": err.Error()})
		return
	}

	if len(outdatedItems) == 0 {
		log.Info("All prices are fresh ‚ú®", nil)
		return
	}

	log.Info("Found outdated items", map[string]interface{}{"count": len(outdatedItems)})

	// –û–±—Ö–æ–¥–∏–º –∏—Ö
	for _, item := range outdatedItems {
		log.Info("Rescraping item", map[string]interface{}{"url": item.URL, "shop_id": item.ShopID})

		// –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –º–∞–≥–∞–∑–∏–Ω–∞
		shopConfig, err := app.GetShopConfig(item.ShopID)
		if err != nil {
			log.Error("Shop config not found", map[string]interface{}{
				"shop_id": item.ShopID,
				"error":   err.Error(),
			})
			continue
		}

		// –ü–∞—Ä—Å–∏–º
		// ScrapeAndSave –æ–±–Ω–æ–≤–ª—è–µ—Ç existing product —á–µ—Ä–µ–∑ Processor.ProcessRawProducts (UPSERT)
		_, err = app.ScraperService.ScrapeAndSave(ctx, item.URL, shopConfig)
		if err != nil {
			log.Error("Rescrape failed", map[string]interface{}{
				"url":   item.URL,
				"error": err.Error(),
			})
		} else {
			log.Info("Rescraped successfully", map[string]interface{}{"url": item.URL})
		}

		// –í–µ–∂–ª–∏–≤–∞—è –ø–∞—É–∑–∞
		time.Sleep(5 * time.Second)
	}

	log.Info("‚úÖ Monitoring scrape completed", map[string]interface{}{"count": len(outdatedItems)})
}
